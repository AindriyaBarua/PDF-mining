{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "84ab3fbf-22a7-4a53-8ebe-ca7afb3b76dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting fasttext\n",
      "  Using cached fasttext-0.9.2-cp36-cp36m-win_amd64.whl\n",
      "Requirement already satisfied: setuptools>=0.7.0 in c:\\users\\ab078357\\.conda\\envs\\minepdf\\lib\\site-packages (from fasttext) (58.0.4)\n",
      "Collecting pybind11>=2.2\n",
      "  Downloading pybind11-2.9.0-py2.py3-none-any.whl (210 kB)\n",
      "Requirement already satisfied: numpy in c:\\users\\ab078357\\.conda\\envs\\minepdf\\lib\\site-packages (from fasttext) (1.19.5)\n",
      "Installing collected packages: pybind11, fasttext\n",
      "Successfully installed fasttext-0.9.2 pybind11-2.9.0\n"
     ]
    }
   ],
   "source": [
    "!pip install fasttext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "123c9ce1-1a2e-4c95-ba78-4ecdfb98847a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\AB078357\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\AB078357\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('wordnet')\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3368b4c8-af7c-4ca5-8546-c0434ffe52d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "import re\n",
    "import fasttext\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "960bab22-5097-41ef-990c-944287c8ab3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def lemmatize_sentence(tokens):\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    lemmatized_tokens = [lemmatizer.lemmatize(word) for word in tokens]\n",
    "    return lemmatized_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "06a35dc3-4e6a-4427-b346-3e2af4130a96",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_and_remove_punctuation(sentence):\n",
    "    tokenizer = RegexpTokenizer(r'\\w+')\n",
    "    tokens = tokenizer.tokenize(sentence)\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f58d5444-5780-46e4-ab9b-97986e691cec",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_stopwords(word_tokens):\n",
    "    filtered_tokens = [word for word in word_tokens if word not in stopwords.words('english')]\n",
    "    return filtered_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a26e744d-f654-41e7-b80b-04b546ee10e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalise_text(text):\n",
    "    sent = text.lower()\n",
    "    \n",
    "    re.sub('\\n+', ' ', sent)\n",
    "    re.sub(' +', ' ', sent)\n",
    "    tokens = tokenize_and_remove_punctuation(sent)\n",
    "    lemmatized_tokens = lemmatize_sentence(tokens)\n",
    "    orig = lemmatized_tokens\n",
    "    filtered_tokens = remove_stopwords(tokens)\n",
    "    if len(filtered_tokens) == 0:\n",
    "        # if stop word removal removes everything, don't do it\n",
    "        filtered_tokens = orig\n",
    "    return filtered_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "82220c17-872b-4316-b4ba-edd7c32702ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "def camel_case_split(tokens):\n",
    "    camel_tokens = []\n",
    "    for token in tokens:\n",
    "        matches = re.finditer('.+?(?:(?<=[a-z])(?=[A-Z])|(?<=[A-Z])(?=[A-Z][a-z])|$)', token)        \n",
    "        token_parts = [m.group(0) for m in matches]\n",
    "        camel_tokens.append(token_parts)\n",
    "    return camel_tokens\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d019f14a-e100-49c0-983b-75037df81efb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_fasttext(txt = 'all_text_for_ft_train.csv'):\n",
    "    ft_model = fasttext.train_unsupervised(txt, model='skipgram', lr=0.05, dim=100, ws=5, epoch=5)\n",
    "    ft_model.save_model(\"fasttext.model\")\n",
    "    return ft_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f33b040b-ac33-4e45-97a1-c1d20d188996",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Act', '1957_10052012']\n"
     ]
    }
   ],
   "source": [
    "def normalise_title(title):\n",
    "    title = title.strip(\".csv\")\n",
    "    title.replace(\"_\", \" \")\n",
    "    tokens = tokenize_and_remove_punctuation(title)\n",
    "    \n",
    "    tokens = camel_case_split(tokens)\n",
    "    \n",
    "    if len(tokens)> 1:\n",
    "        \n",
    "        tokens = [item for sublist in tokens for item in sublist]\n",
    "    else:\n",
    "        tokens = tokens[0]\n",
    "    return tokens\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5c1ebffc-4936-418f-b941-39e383e97658",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_title_vector(title_tokens, ft_model):\n",
    "    vec = np.mean([ft_model[word] for word in title_tokens], axis=0)\n",
    "    return vec\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e714be78-c0dd-4b6d-a6ee-b1de97c50c94",
   "metadata": {},
   "outputs": [],
   "source": [
    "high_weight_words = {'report':2, 'tp':3, 'target':8, 'price': 4, 'reco':5 , 'recommendation':8 , 'sell':9, 'buy':9, 'hold':9, 'increase':9, 'reduce':9, 'rating':9, 'review':2, 'fiscal':2, 'stock':3, 'share':3}\n",
    "\n",
    "def get_text_vector(text_tokens, ft_model):\n",
    "    sum = 0\n",
    "    '''\n",
    "    if is_page_text:\n",
    "        tax = 0.5\n",
    "    else:\n",
    "        tax = 1'''\n",
    "    for token in text_tokens:\n",
    "        try:\n",
    "            ft_vec = ft_model[token]\n",
    "        except:\n",
    "            ft_vec = [0] * 100\n",
    "        if token.lower() in high_weight_words.keys():\n",
    "            #tax = 1 # if any of these words is there, then no tax\n",
    "            vec = ft_vec * high_weight_words.get(token.lower())\n",
    "        else:\n",
    "            vec = ft_vec \n",
    "        sum = sum + vec\n",
    "    if len(text_tokens)>0:\n",
    "        mean = sum/len(text_tokens)\n",
    "    else:\n",
    "        mean = [0] * 100\n",
    "            \n",
    "    return mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "203e0df2-7dd7-4930-9851-033609ced304",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_doc_vector(text_vec, title_vec):\n",
    "    doc_vec = np.concatenate((text_vec, title_vec), axis = None)\n",
    "    return doc_vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "3be9c6e3-6ef5-446e-8a1c-38045d0484b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.cluster import KMeans\n",
    "def cluster_docs(doc_vecs):\n",
    "    true_k = 2\n",
    "    model = KMeans(n_clusters=true_k, init='k-means++', max_iter=100, n_init=1)\n",
    "    model.fit(X)\n",
    "    order_centroids = model.cluster_centers_.argsort()[:, ::-1]\n",
    "    terms = vectorizer.get_feature_names()\n",
    "    for i in range(true_k):\n",
    "        print(\"Cluster:\", i)\n",
    "        for ind in order_centroids[i, :20]:\n",
    "             print(terms[ind])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "2b347bd3-7c31-40d4-8e22-67a0a4dc9f8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "def write_all_txt_file(pdf_texts):\n",
    "    all_text_list = [item for sublist in pdf_texts for item in sublist]            \n",
    "    file = open('all_text_for_ft_train.csv', 'w+', newline ='\\n', encoding = \"utf-8\")\n",
    "    with file:\n",
    "        write = csv.writer(file)\n",
    "        write.writerows(map(lambda x: [x], all_text_list))\n",
    "write_all_txt_file([[\"page1a\", \"page1a\"], [\"page2a\", \"page2b\"]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "c8e7844c-5dc3-4407-9c4a-2539476a0c1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from os import listdir\n",
    "from os.path import join\n",
    "def load_pdf_texts():\n",
    "    textpath = \"needl\\\\pagewise_folder\"\n",
    "    files = [f for f in listdir(textpath) if f.endswith(\".csv\")]\n",
    "    pdf_texts = []\n",
    "    \n",
    "    for file in files:\n",
    "        df = pd.read_csv(join(textpath, file))\n",
    "        # buy sell target is always in the first page! But we can train Fasttext on more text, so here we can take full text\n",
    "        pdf_text = df['page_text'].to_list() # list of pages            \n",
    "        pdf_text = [(re.sub(r'\\n+ +', '', str(page))) for page in pdf_text]\n",
    "        pdf_text = [(re.sub(r'\\n|-+|_+', '', page)).strip() for page in pdf_text]\n",
    "        pdf_text = [(re.sub(r'\\t* +', ' ', page)).strip() for page in pdf_text] #list of pages: text of each page in a pdf, comma seperated\n",
    "        pdf_texts.append(pdf_text)\n",
    "        \n",
    "    return files, pdf_texts\n",
    "#load_pdf_texts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "8abd2b14-47e1-4ee6-9265-b79535ac89e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "590\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "pdf_titles, pdf_texts = load_pdf_texts()\n",
    "print(len(pdf_texts))\n",
    "write_all_txt_file(pdf_texts)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "e68a148a-4eb8-4a70-8454-8de6334d63eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "ft_model = train_fasttext()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "fd206204-e24d-4c1d-8e9c-383ad0c49235",
   "metadata": {},
   "outputs": [],
   "source": [
    "index = 0\n",
    "doc_vecs = []\n",
    "all_text_tokens = []\n",
    "all_title_tokens = []\n",
    "for filename in pdf_titles:\n",
    "    title_tokens = normalise_title(filename)\n",
    "    all_title_tokens.append(title_tokens)\n",
    "    text_tokens = normalise_text(\" \".join(pdf_texts[index][:min(len(pdf_texts[index])-1, 2)])) #list of pages in one file--> take first 2 pages as all reports have it in first\n",
    "    all_text_tokens.append(text_tokens)\n",
    "    title_vec = get_title_vector(title_tokens, ft_model)\n",
    "    text_vec = get_text_vector(text_tokens, ft_model)\n",
    "    doc_vec = get_doc_vector(text_vec, title_vec)\n",
    "    doc_vecs.append(doc_vec)\n",
    "    index = index + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "91146f5e-31c9-438e-ad84-967f58499a62",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = [np.array(doc) for doc in doc_vecs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "6c4a4f69-5165-4cd5-8349-1114ec65723b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(590, 200)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.shape(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "58920db9-f9d0-4a7f-bbce-e7470413429c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([ 0.1921955 , -0.12435074, -0.21393298, -0.6243164 ,  0.00340513,\n",
       "        -0.31052667,  0.18717957,  0.1272794 ,  0.49035332,  0.31263176,\n",
       "         0.01808154,  0.150024  ,  0.25035203,  0.25768116, -0.18416847,\n",
       "        -0.05810773,  0.2820143 , -0.24629155,  0.19242777,  0.24783288,\n",
       "         0.12158884, -0.05855266, -0.062657  ,  0.11838558, -0.419708  ,\n",
       "        -0.05644286, -0.14761865,  0.4465786 , -0.27247304,  0.10655572,\n",
       "        -0.24593326,  0.11961447,  0.23427354, -0.09474812,  0.3012488 ,\n",
       "        -0.11113767, -0.26062927, -0.5127111 , -0.26735786, -0.3927003 ,\n",
       "        -0.21719971,  0.09702367,  0.02819763, -0.02862808, -0.01238478,\n",
       "        -0.19762309, -0.02476684, -0.02394044, -0.100161  , -0.26864403,\n",
       "        -0.15595084, -0.13729201, -0.2451535 , -0.32944354,  0.20419429,\n",
       "         0.08596294,  0.13392596, -0.27922717, -0.5627646 , -0.24780582,\n",
       "         0.35519147,  0.11456683,  0.05126396,  0.05663646, -0.6238701 ,\n",
       "         0.2333658 ,  0.35012138, -0.00503631,  0.01550995, -0.01911478,\n",
       "        -0.5557726 , -0.06092749, -0.31231707, -0.21589997,  0.11996893,\n",
       "         0.17838022,  0.21467395, -0.21015038, -0.02888909,  0.01247989,\n",
       "         0.09969591,  0.1378953 , -0.30778256,  0.08634201,  0.21042046,\n",
       "         0.05217224,  0.17140713, -0.00477383, -0.646737  ,  0.2192925 ,\n",
       "         0.11362773,  0.1900299 ,  0.14512354, -0.26745453,  0.02811046,\n",
       "        -0.13463606, -0.12053339, -0.20932269, -0.24650477,  0.34244835,\n",
       "         0.06199074, -0.07171561, -0.20496926, -0.47476828, -0.07253581,\n",
       "        -0.23147239,  0.09633756,  0.19512549,  0.4019193 ,  0.18499756,\n",
       "         0.08287849,  0.26641697, -0.00457417,  0.09114511, -0.13970259,\n",
       "         0.16719356,  0.1131122 , -0.2333874 ,  0.02941765,  0.12268268,\n",
       "         0.29222503, -0.14182764,  0.01969604,  0.28707063, -0.2294083 ,\n",
       "        -0.2032813 , -0.11687357,  0.16013291, -0.44514596,  0.10433528,\n",
       "        -0.33053723,  0.08386599,  0.0459723 ,  0.17657274,  0.1169581 ,\n",
       "         0.14856185, -0.1372756 , -0.16714524, -0.2539485 , -0.19464178,\n",
       "        -0.11239336,  0.18487994, -0.03102736,  0.12303606, -0.22159007,\n",
       "        -0.04689017,  0.00915851,  0.08020496, -0.05961327, -0.07383652,\n",
       "        -0.18185803, -0.04385753, -0.24052443, -0.15446313,  0.2904817 ,\n",
       "         0.05508002,  0.10692991, -0.01761697, -0.36339703, -0.27405033,\n",
       "         0.53584206,  0.02870228,  0.2856306 , -0.02924463, -0.35028684,\n",
       "        -0.04702272,  0.22134686,  0.16302961, -0.11598388, -0.21046558,\n",
       "        -0.2954044 , -0.1584322 , -0.0379672 , -0.38037583, -0.00763512,\n",
       "        -0.22919741, -0.09732497, -0.08295253, -0.08024681,  0.1086172 ,\n",
       "         0.17527884,  0.1013993 , -0.13992302, -0.04552038,  0.00432895,\n",
       "         0.1022328 ,  0.2851125 , -0.10851552, -0.4589376 ,  0.08013374,\n",
       "         0.2832625 ,  0.14659482,  0.01417857, -0.69983834,  0.02373262,\n",
       "        -0.06785446, -0.21287715, -0.22190648, -0.21158123,  0.19262266],\n",
       "       dtype=float32),\n",
       " array([ 3.69358480e-01,  1.52088463e-01, -1.23124972e-01, -3.70818824e-01,\n",
       "         2.22894382e-02, -2.01319993e-01,  1.94515944e-01,  2.32835948e-01,\n",
       "         2.44211137e-01,  6.23564899e-01, -1.80586055e-02,  1.07519433e-01,\n",
       "         3.11896324e-01,  1.92180738e-01, -1.59019321e-01, -8.21107030e-02,\n",
       "         4.78792071e-01, -2.70789474e-01,  5.90883680e-02,  2.75477409e-01,\n",
       "        -3.59266132e-01, -6.32188693e-02, -1.93356857e-01,  4.36382368e-04,\n",
       "        -2.51392454e-01, -2.47940738e-02, -1.85223401e-01,  3.52498919e-01,\n",
       "        -1.41195476e-01,  7.21701756e-02, -2.93513238e-01,  1.64912730e-01,\n",
       "        -8.42041299e-02,  1.13432378e-01,  9.18720141e-02,  1.68786183e-01,\n",
       "        -2.49936655e-01, -2.88580000e-01, -1.44263089e-01, -3.29687655e-01,\n",
       "        -2.60429949e-01,  1.39534444e-01, -1.22267567e-03,  3.34466159e-01,\n",
       "         1.78527504e-01, -1.06260248e-01, -7.02327341e-02,  1.14419818e-01,\n",
       "         3.59615032e-03, -7.56084546e-02, -2.49183774e-02,  2.38917209e-02,\n",
       "        -1.05465762e-01, -2.05035269e-01, -1.88737974e-01,  1.97489128e-01,\n",
       "         2.86040783e-01, -3.03807646e-01, -1.16451636e-01, -1.44340456e-01,\n",
       "         1.88199982e-01,  1.32816553e-01,  9.02780145e-02, -1.27905741e-01,\n",
       "        -2.00723588e-01,  1.72965631e-01,  2.89203733e-01, -1.21042356e-01,\n",
       "        -2.34161481e-01, -7.05229770e-03, -3.91345620e-01, -2.96796951e-03,\n",
       "        -3.01362842e-01,  2.22055484e-02,  6.76579922e-02, -1.10858902e-01,\n",
       "         3.59742999e-01,  7.68579543e-02,  4.81445640e-02,  4.40995023e-03,\n",
       "         1.36523601e-03,  1.30068034e-01, -3.64483774e-01,  4.74736005e-01,\n",
       "         1.83940440e-01,  3.12022507e-01,  4.29328322e-01,  4.98783253e-02,\n",
       "        -3.72850239e-01,  2.24278450e-01,  2.62340337e-01,  1.71062291e-01,\n",
       "        -1.59969270e-01, -1.80594027e-01,  6.79825526e-03, -4.79298830e-02,\n",
       "         1.76290482e-01, -2.16209851e-02, -7.99563676e-02,  2.96628743e-01,\n",
       "         2.44600713e-01,  6.61680475e-02, -6.85500130e-02, -1.57101870e-01,\n",
       "         6.40423223e-02, -2.56388843e-01,  8.54635090e-02,  1.16872847e-01,\n",
       "        -2.58136597e-02,  4.37928140e-01,  2.27307856e-01,  2.44623423e-03,\n",
       "         7.05891103e-02,  8.62956196e-02, -1.54339075e-01, -1.05021978e-02,\n",
       "         4.75812703e-01, -2.19316199e-01, -1.13061495e-01,  1.57159179e-01,\n",
       "        -5.53805642e-02, -3.93463299e-02,  4.74249683e-02,  1.24261439e-01,\n",
       "        -1.50783569e-01,  1.22996308e-01, -1.36410259e-02, -4.58446704e-02,\n",
       "        -3.25519234e-01,  1.99129563e-02, -2.39516586e-01,  4.73970808e-02,\n",
       "        -1.22713841e-01,  2.74610162e-01,  6.43437058e-02,  6.30062893e-02,\n",
       "        -1.59193814e-01, -1.01480588e-01, -3.93534452e-02, -4.41152513e-01,\n",
       "        -2.91638404e-01,  1.37850806e-01,  1.50481081e-02,  1.36674061e-01,\n",
       "         8.55178088e-02, -1.61834583e-01, -2.54895091e-01, -2.56051924e-02,\n",
       "         5.94363138e-02, -7.06320331e-02, -5.13005704e-02,  3.97155769e-02,\n",
       "        -1.88726008e-01, -1.12309992e-01, -8.50398745e-03,  8.52391645e-02,\n",
       "         2.32048914e-01, -1.95543885e-01, -1.89491287e-01, -1.29425824e-01,\n",
       "         3.33339721e-01, -2.39174794e-02,  2.64381528e-01, -1.44700184e-01,\n",
       "        -1.23590313e-01, -3.66993174e-02,  1.08855657e-01,  1.38640389e-01,\n",
       "        -9.68337059e-02, -1.65160205e-02, -1.76061049e-01, -1.20423175e-01,\n",
       "        -2.28329912e-01,  4.00018319e-03,  4.05967981e-03, -2.26214305e-01,\n",
       "         1.61643699e-01,  1.31439611e-01, -4.12357338e-02,  1.18690215e-01,\n",
       "        -7.08362684e-02,  1.56938106e-01, -5.75466268e-03,  4.04940099e-01,\n",
       "         1.44567475e-01,  1.31829008e-01,  4.28553522e-01, -1.22706778e-01,\n",
       "        -3.57192308e-01,  2.53915098e-02,  8.71383101e-02,  7.37735927e-02,\n",
       "        -3.04305971e-01, -2.25545153e-01, -1.88080799e-02, -1.30283818e-01,\n",
       "         1.03675686e-01,  7.75393918e-02, -1.57710165e-01,  6.66318312e-02],\n",
       "       dtype=float32)]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X[:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "75ee942f-29f8-468b-bc12-995a9d2bb8b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "true_k = 2\n",
    "model = KMeans(n_clusters=true_k, init='k-means++', max_iter=300, n_init=10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "d4e0bd7d-c14f-4ae7-a264-ecd5fe15962b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "KMeans(n_clusters=2)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "fb53b30f-da17-4e41-aa21-a7af9dc7c2f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# since the vectors are not just of words, we cant print words in each cluster to identify what each cluster signifies\n",
    "# so we can take 10 manually chosen reports and see in which cluster they fall and use that as reports cluster-- take frequency, one in which max reports fall\n",
    "manual_reports = ['[Kotak] Godrej Properties, January 29, 2019.pdf', '160125_Huchems_Final (2).pdf', '20141013_IndusInd-Bank-Limited_204_QuarterUpdate.pdf', 'Adani_Power_Q1FY18_results.pdf', 'BP Equities- National Buildings Construction Corporation Ltd NBCC- Short note_11th April, 2014.pdf', 'Exide_4QFY16 Results Review Final.pdf', 'HDFC Securities 16-May-16.pdf', 'IDirect_Greenply_IC.pdf', 'IIFL-+Titan-Gold+standard-ADD.pdf', 'Mahindra and Mahindra Financial Services-result update-Jul-13-EDEL.pdf']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "e1e84098-a72d-4cdd-9099-221567beabfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def check(files):\n",
    "    index = 0\n",
    "    input_vectors = []\n",
    "    for file in files:\n",
    "        if file.endswith(\".pdf\"):\n",
    "            file = file.strip(\".pdf\")+\".csv\"\n",
    "        index = pdf_titles.index(file)\n",
    "        doc_vec = doc_vecs[index]\n",
    "        input_vectors.append(doc_vec)\n",
    "    tags = model.predict(input_vectors)\n",
    "    #all known reports are clustered in 0\n",
    "    unique, counts = np.unique(tags, return_counts=True)\n",
    "    print(dict(zip(unique, counts)))\n",
    "    return tags\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "fc041f07-55c8-4cc0-b827-670f8e912559",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0: 10}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0])"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "check(manual_reports) # Pdfs we surely know are reports by manual checking are falling 0 cluster, so report = cluster 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "92035093-6ba8-4b90-986c-89ebb22e7b9c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0: 571, 1: 19}\n"
     ]
    }
   ],
   "source": [
    "tags = check(pdf_titles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "a45cf336-5424-4d6d-9f22-7b84d176f694",
   "metadata": {},
   "outputs": [],
   "source": [
    "import collections\n",
    "\n",
    "def search_recomendation(tokens):\n",
    "    tokens = [token.lower() for token in tokens]\n",
    "    index_dict = {'buy':-1, 'sell':-1, 'hold':-1, 'increase':-1, 'decrease':-1}\n",
    "    try:\n",
    "        index_dict['buy'] = tokens.index(\"buy\")\n",
    "    except:\n",
    "        pass\n",
    "    try:\n",
    "        index_dict['sell'] = tokens.index(\"sell\")\n",
    "    except:\n",
    "        pass\n",
    "    try:\n",
    "        index_dict['hold'] = tokens.index(\"hold\")\n",
    "    except:\n",
    "        pass\n",
    "    try:\n",
    "        index_dict['increase'] = tokens.index(\"increase\")\n",
    "    except:\n",
    "        pass\n",
    "    try:\n",
    "        index_dict['decrease'] = tokens.index(\"decrease\")\n",
    "    except:\n",
    "        pass\n",
    "    \n",
    "    if not (all(value == -1 for value in index_dict.values())):\n",
    "        index_dict = {key:val for key, val in index_dict.items() if val != -1} \n",
    "        recomendation = min(index_dict, key=index_dict.get)\n",
    "        return recomendation\n",
    "    else:   \n",
    "        return \"NA\"\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "id": "62a9e456-2d0c-4c31-b6f1-8e53657d2e79",
   "metadata": {},
   "outputs": [],
   "source": [
    "def search_targetprice(full_text):\n",
    "    \n",
    "    matches = re.findall(r\"(?i)(target price|TP).*\\d+\", full_text[0])\n",
    "    # in between target price and the value, there may be words like \"TP amounts to INR 123\", or \"TP: INR123\"\n",
    "    # but this .* takes everything upto the last digit in text\n",
    "    if matches != []:\n",
    "        phrase = matches[0]\n",
    "        values = re.findall(\"...\\d+\", phrase) # in case of \"TP '234\" there 3 characters to catch the currency -- INR, W, RS etc\n",
    "        \n",
    "        if len(values) > 0:\n",
    "            value = re.sub(': ', '', values[0]).strip()\n",
    "            ret = value\n",
    "        \n",
    "        else:\n",
    "            try:\n",
    "                digit_matches = re.findall('\\d+', phrase)[0].strip()\n",
    "                ret = digit_matches[0]\n",
    "            except:\n",
    "                ret = \"NA\"\n",
    "            \n",
    "    else:\n",
    "        ret = \"NA\"\n",
    "    return ret"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "id": "592d8459-4bff-4b05-97b1-2c5e8d8c9d30",
   "metadata": {},
   "outputs": [],
   "source": [
    "def search_details(filename, index):\n",
    "    report_info = {}\n",
    "    full_text = pdf_texts[index]\n",
    "    text_tokens = all_text_tokens[index]\n",
    "    title_tokens = all_title_tokens[index]\n",
    "    recomendation = search_recomendation(text_tokens)\n",
    "    target = search_targetprice(full_text)\n",
    "    return recomendation, target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "id": "036cf416-5a50-47d6-b9da-52f24238a582",
   "metadata": {},
   "outputs": [],
   "source": [
    "index = 0\n",
    "score = 0\n",
    "reco = ['buy', 'sell', 'hold', 'increase', 'decrease']\n",
    "report_mined = []\n",
    "recommendation = ''\n",
    "c = 1\n",
    "for filename in pdf_titles:\n",
    "    searched = 0\n",
    "    #smallest subset\n",
    "    for token in title_tokens:\n",
    "        if token in reco:\n",
    "            recommendation, target = search_details(filename, index)\n",
    "            searched = 1\n",
    "            break\n",
    "    if searched == 0:\n",
    "        for token in text_tokens:\n",
    "            if token in high_weight_words.keys():\n",
    "                recommendation, target = search_details(filename, index)\n",
    "                \n",
    "                searched = 1\n",
    "                break\n",
    "        if searched == 0:\n",
    "            if tags[index] == 0: # in reports cluster\n",
    "                searched == 1\n",
    "                recommendation, target = search_details(filename, index)\n",
    "    if searched == 1 and recommendation != \"NA\":\n",
    "        report_mined.append({\"file\": filename.strip(\".csv\"), \"reco\": recommendation, \"target price\": target})\n",
    "        all_text_tokens[index]\n",
    "        \n",
    "    index = index + 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "id": "bba5e791-b2a1-45a1-84a9-8c68eeb92b3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "with open('report_extracted.json', 'w') as file:\n",
    "    json.dump(report_mined, file, indent = 4 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f966fff-0f6d-4828-9a12-3ca5dfebff18",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
